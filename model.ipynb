{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPS1bXtCz95GKJPjbQtiqLl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaddo82/AI-/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqUmvTUqG58F",
        "outputId": "151f1815-e641-4ac7-8d08-a6f8d951da22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (0) 드라이브 마운트\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# (1) 임포트/기본 설정\n",
        "from pathlib import Path\n",
        "import os, time, math, gc, random, warnings, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from contextlib import nullcontext\n",
        "import torchaudio\n",
        "import torchaudio.functional as AF\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    classification_report, accuracy_score, precision_recall_fscore_support,\n",
        "    confusion_matrix, roc_auc_score, average_precision_score,\n",
        "    precision_recall_curve, roc_curve\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*TorchAudio.*deprecated.*\")\n",
        "warnings.filterwarnings(\"ignore\", message=\".*torchcodec.*\")\n",
        "warnings.filterwarnings(\"default\")\n",
        "\n",
        "try:\n",
        "    mp.set_start_method(\"spawn\", force=True)\n",
        "except RuntimeError:\n",
        "    pass\n",
        "\n",
        "# ===== 경로/하이퍼 =====\n",
        "SAVE_DIR   = \"/content/drive/MyDrive/tts_outputs\"\n",
        "\n",
        "# 데이터 루트들 (필요시 수정)\n",
        "EDGE_ROOT  = Path(\"/content/drive/MyDrive/tts_outputs/edge_tts\")       # tts\n",
        "GSM_ROOT   = Path(\"/content/drive/MyDrive/tts_outputs/edge_tts_gsm\")   # tts_gsm\n",
        "ORIG_ROOT  = Path(\"/content/drive/MyDrive/origin\")                      # orig\n",
        "\n",
        "# 빠른 메타 스캔 옵션\n",
        "FAST_SCAN       = True\n",
        "MAX_CSV_PER_DIR = 3\n",
        "\n",
        "# 오디오/로더\n",
        "TARGET_SR       = 16000\n",
        "FIXED_SECONDS   = 2\n",
        "FIXED_SAMPLES   = TARGET_SR * FIXED_SECONDS\n",
        "\n",
        "SAMPLE_FRAC     = 1.0\n",
        "MAX_PER_CLASS   = None\n",
        "\n",
        "# 3-way split\n",
        "TRAIN_FRAC, VAL_FRAC, TEST_FRAC = 0.80, 0.20, 0.00\n",
        "\n",
        "# 학습\n",
        "EPOCHS          = 5\n",
        "BATCH_SIZE      = 16\n",
        "VAL_BATCH_SIZE  = 16\n",
        "NUM_WORKERS_TR  = 0\n",
        "NUM_WORKERS_VA  = 0\n",
        "LOG_EVERY_SEC   = 2\n",
        "EMPTY_CACHE_EVERY = 400\n",
        "SEED            = 42\n",
        "\n",
        "# 모델\n",
        "FREEZE_BACKBONE = False\n",
        "LR_HEAD         = 5e-5\n",
        "LR_FULL         = 1e-6\n",
        "\n",
        "GRAD_CLIP = 1.0\n",
        "WARMUP_STEPS = 500\n",
        "\n",
        "# 3라벨 고정\n",
        "class_names = [\"orig\", \"tts\", \"tts_gsm\"]\n",
        "label_map   = {c:i for i,c in enumerate(class_names)}\n",
        "id2label    = {i:c for i,c in enumerate(class_names)}\n",
        "label2id    = {v:k for k,v in id2label.items()}\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# (2) ORIG metadata.csv 자동 생성\n",
        "def ensure_orig_metadata_csv(orig_root: Path):\n",
        "    if not orig_root or not orig_root.exists():\n",
        "        print(f\"[orig] 경로가 없습니다: {orig_root}\")\n",
        "        return\n",
        "    csv_path = orig_root / \"metadata.csv\"\n",
        "    if csv_path.exists():\n",
        "        print(f\"[orig] metadata.csv 존재: {csv_path}\")\n",
        "        return\n",
        "    # orig 폴더의 .wav를 재귀로 수집\n",
        "    wavs = sorted(orig_root.rglob(\"*.wav\"))\n",
        "    if not wavs:\n",
        "        print(f\"[orig] wav 파일을 찾지 못했습니다: {orig_root}\")\n",
        "        return\n",
        "    df = pd.DataFrame({\"path\": [str(p) for p in wavs], \"type\": [\"orig\"]*len(wavs)})\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"[orig] metadata.csv 생성 완료: {csv_path}  files={len(df)}\")\n",
        "\n",
        "ensure_orig_metadata_csv(ORIG_ROOT)\n",
        "\n",
        "# (3) 메타데이터 로더\n",
        "def _pick_some_csvs(root: Path, max_csv=3):\n",
        "    root_csv = root / \"metadata.csv\"\n",
        "    if root_csv.exists():\n",
        "        return [root_csv]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "\n",
        "def _read_some_paths(csv_paths, default_type):\n",
        "    out = []\n",
        "    for csv in csv_paths:\n",
        "        try:\n",
        "            df = pd.read_csv(csv)\n",
        "            if \"path\" not in df.columns:\n",
        "                continue\n",
        "            df[\"path\"] = df[\"path\"].astype(str).str.replace(\"//\",\"/\")\n",
        "            if \"type\" not in df.columns:\n",
        "                df[\"type\"] = default_type\n",
        "            out.append(df[[\"path\",\"type\"]])\n",
        "        except Exception:\n",
        "            pass\n",
        "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(columns=[\"path\",\"type\"])\n",
        "\n",
        "def read_all_metadata_3class():\n",
        "    metas = []\n",
        "    for root, default_type in [\n",
        "        (ORIG_ROOT, \"orig\"),\n",
        "        (EDGE_ROOT, \"tts\"),\n",
        "        (GSM_ROOT,  \"tts_gsm\"),\n",
        "    ]:\n",
        "        if not root or not root.exists():\n",
        "            print(f\"[scan] skip (not exists): {root}\")\n",
        "            continue\n",
        "        if FAST_SCAN:\n",
        "            csvs = _pick_some_csvs(root, max_csv=MAX_CSV_PER_DIR)\n",
        "            metas.append(_read_some_paths(csvs, default_type))\n",
        "        else:\n",
        "            pass\n",
        "    if not metas:\n",
        "        raise FileNotFoundError(\"metadata.csv들을 찾지 못했습니다. 루트 경로/파일을 확인하세요.\")\n",
        "    m = pd.concat(metas, ignore_index=True).drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
        "\n",
        "    # 유형 정규화\n",
        "    def norm_type(row):\n",
        "        t = str(row.get(\"type\",\"\")).lower()\n",
        "        p = str(row.get(\"path\",\"\")).lower()\n",
        "        if \"gsm\" in t or p.endswith(\"_gsm.wav\"): return \"tts_gsm\"\n",
        "        if t in {\"orig\",\"original\",\"human\"} or (\"원본\" in p): return \"orig\"\n",
        "        if \"tts\" in t or p.endswith(\"_edge.wav\"): return \"tts\"\n",
        "        # 기본값은 파일 출처에 따라 추정\n",
        "        if \"tts\" in p and \"gsm\" in p: return \"tts_gsm\"\n",
        "        if \"tts\" in p: return \"tts\"\n",
        "        return \"orig\"\n",
        "    m[\"type\"] = m.apply(norm_type, axis=1)\n",
        "\n",
        "    # 존재/크기 필터 + 3라벨만 유지\n",
        "    m = m[m[\"type\"].isin(class_names)].copy()\n",
        "    return m\n",
        "\n",
        "def balance_classes(df, strategy=\"min\"):\n",
        "    counts = df[\"type\"].value_counts().to_dict()\n",
        "    print(\"[before balance]\", counts)\n",
        "\n",
        "    if strategy == \"min\":\n",
        "        target = min(counts.values())\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported strategy\")\n",
        "\n",
        "    outs = []\n",
        "    for t, g in df.groupby(\"type\", group_keys=False):\n",
        "        outs.append(g.sample(n=target, random_state=SEED))\n",
        "    df_bal = pd.concat(outs, ignore_index=True)\n",
        "    print(\"[after balance]\", df_bal[\"type\"].value_counts().to_dict())\n",
        "    return df_bal\n",
        "\n",
        "m = read_all_metadata_3class()\n",
        "m = balance_classes(m, strategy=\"min\")\n",
        "print(f\"[labels] kept types:\", m[\"type\"].value_counts().to_dict())\n",
        "print(f\"[total] {len(m):,} rows  FAST_SCAN={FAST_SCAN}\")\n",
        "\n",
        "# (4) 라벨별 샘플링 → 3-way 분할\n",
        "def stratified_sample(df, frac=0.6, max_per_class=None, seed=42):\n",
        "    outs=[]\n",
        "    for lbl, g in df.groupby(\"type\", group_keys=False):\n",
        "        k = int(len(g)*frac)\n",
        "        g2 = g.sample(n=k, random_state=seed) if k < len(g) else g\n",
        "        if max_per_class is not None and len(g2) > max_per_class:\n",
        "            g2 = g2.sample(n=max_per_class, random_state=seed)\n",
        "        outs.append(g2)\n",
        "    return pd.concat(outs, ignore_index=True) if outs else pd.DataFrame(columns=df.columns)\n",
        "\n",
        "def stratified_split_3(df, train_frac, val_frac, test_frac, seed=42):\n",
        "    assert abs((train_frac+val_frac+test_frac)-1.0) < 1e-6\n",
        "\n",
        "    parts=[]\n",
        "    for lbl, g in df.groupby(\"type\", group_keys=False):\n",
        "        g = g.sample(frac=1.0, random_state=seed)\n",
        "        n = len(g)\n",
        "        n_tr = int(n * train_frac)\n",
        "        n_val = int(n * val_frac)\n",
        "        n_te = int(n * test_frac)\n",
        "\n",
        "\n",
        "        if test_frac == 0:\n",
        "            n_te = 0\n",
        "\n",
        "        parts.append((\n",
        "            g.iloc[:n_tr].assign(_split=\"train\"),\n",
        "            g.iloc[n_tr:n_tr+n_val].assign(_split=\"val\"),\n",
        "            g.iloc[n_tr+n_val:n_tr+n_val+n_te].assign(_split=\"test\"),\n",
        "        ))\n",
        "\n",
        "    out = pd.concat([p for trio in parts for p in trio], ignore_index=True)\n",
        "    return (\n",
        "        out[out[\"_split\"]==\"train\"].drop(columns=[\"_split\"]).reset_index(drop=True),\n",
        "        out[out[\"_split\"]==\"val\"].drop(columns=[\"_split\"]).reset_index(drop=True),\n",
        "        out[out[\"_split\"]==\"test\"].drop(columns=[\"_split\"]).reset_index(drop=True),\n",
        "    )\n",
        "\n",
        "\n",
        "# 오디오 Augment 예시 (학습 시 적용)\n",
        "def augment_audio(wav, sr):\n",
        "    noise = torch.randn(len(wav), dtype=torch.float32) * 0.005\n",
        "    wav2 = wav.float() + noise\n",
        "    wav2 = torch.clamp(wav2, -1.0, 1.0)\n",
        "    return wav2.float()\n",
        "\n",
        "\n",
        "\n",
        "m = stratified_sample(m, frac=SAMPLE_FRAC, max_per_class=MAX_PER_CLASS, seed=SEED)\n",
        "assert not m.empty, \"샘플링 결과가 비었습니다. SAMPLE_FRAC을 키우거나 경로/메타를 확인하세요.\"\n",
        "df_tr, df_val, df_te = stratified_split_3(m, TRAIN_FRAC, VAL_FRAC, TEST_FRAC, seed=SEED)\n",
        "print(f\"[split] train={len(df_tr):,}  val={len(df_val):,}  test={len(df_te):,}\")\n",
        "print(\"[by_type] train:\", df_tr[\"type\"].value_counts().to_dict())\n",
        "print(\"[by_type] val  :\", df_val[\"type\"].value_counts().to_dict())\n",
        "print(\"[by_type] test :\", df_te[\"type\"].value_counts().to_dict())\n",
        "\n",
        "# (5) WAV 로더 유틸 (16k 모노, 고정길이 랜덤/센터 크롭)\n",
        "def load_wav_16k_mono(path, retry=1):\n",
        "    # soundfile 우선 → torchaudio 백업\n",
        "    last_err = None\n",
        "    for _ in range(max(1, retry+1)):\n",
        "        try:\n",
        "            data, sr = sf.read(path, dtype=\"float32\", always_2d=False)\n",
        "            wav = torch.from_numpy(data)\n",
        "            if wav.ndim > 1:\n",
        "                wav = wav.mean(dim=-1)\n",
        "            if sr != TARGET_SR:\n",
        "                wav = AF.resample(wav.unsqueeze(0), sr, TARGET_SR).squeeze(0)\n",
        "            return wav\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            try:\n",
        "                w, sr = torchaudio.load(path)\n",
        "                if w.dtype != torch.float32:\n",
        "                    w = w.to(torch.float32)\n",
        "                if w.size(0) > 1:\n",
        "                    w = w.mean(dim=0, keepdim=True)\n",
        "                if sr != TARGET_SR:\n",
        "                    w = AF.resample(w, sr, TARGET_SR)\n",
        "                return w.squeeze(0)\n",
        "            except Exception as e2:\n",
        "                last_err = e2\n",
        "                time.sleep(0.03)\n",
        "    return None\n",
        "\n",
        "def fixed_length_random(wav: torch.Tensor, fixed_len: int = FIXED_SAMPLES):\n",
        "    wav = wav.view(-1)\n",
        "    n = wav.numel()\n",
        "    if n == fixed_len: return wav\n",
        "    if n > fixed_len:\n",
        "        start = random.randint(0, n - fixed_len)\n",
        "        return wav[start:start+fixed_len]\n",
        "    out = torch.zeros(fixed_len, dtype=wav.dtype)\n",
        "    out[:n] = wav\n",
        "    return out\n",
        "\n",
        "def make_loader_from_wavs(df, batch_size, shuffle, num_workers, pin_memory=True, augment=False):\n",
        "    if df is None or df.empty:\n",
        "        return None\n",
        "    paths  = df[\"path\"].tolist()\n",
        "    labels = df[\"type\"].tolist()\n",
        "\n",
        "    class _DS(Dataset):\n",
        "        def __len__(self): return len(paths)\n",
        "        def __getitem__(self, i):\n",
        "            if i % 1000 == 0:\n",
        "                print(f\"[WAV-LOAD] idx={i}/{len(paths)}\")\n",
        "            p = paths[i]; t = labels[i]\n",
        "            try:\n",
        "                if (not os.path.exists(p)) or os.path.getsize(p) < 100:\n",
        "                    return None, None\n",
        "            except Exception:\n",
        "                return None, None\n",
        "            wav = load_wav_16k_mono(p, retry=1)\n",
        "            if wav is None:\n",
        "                return None, None\n",
        "            wav = fixed_length_random(wav, FIXED_SAMPLES)\n",
        "            if augment:\n",
        "                if random.random() < 0.3:\n",
        "                    wav = augment_audio(wav, TARGET_SR)\n",
        "            return wav, label_map[t]\n",
        "\n",
        "    def _collate(batch):\n",
        "        keep = [(w,l) for (w,l) in batch if (w is not None and l is not None)]\n",
        "        if not keep:\n",
        "            xb = torch.zeros((1, FIXED_SAMPLES), dtype=torch.float32)\n",
        "            yb = torch.zeros((1,), dtype=torch.long)\n",
        "            am = torch.ones((1, FIXED_SAMPLES), dtype=torch.long)\n",
        "            return {\"input_values\": xb, \"attention_mask\": am, \"labels\": yb, \"empty\": True}\n",
        "        waves, lbls = zip(*keep)\n",
        "        xb = torch.stack(waves, dim=0)\n",
        "        yb = torch.tensor(lbls, dtype=torch.long)\n",
        "        am = torch.ones((xb.size(0), xb.size(1)), dtype=torch.long)\n",
        "        return {\"input_values\": xb, \"attention_mask\": am, \"labels\": yb, \"empty\": False}\n",
        "\n",
        "    # num_workers=0 (prefetch_factor 미지정) → 첫 배치 정체 방지\n",
        "    return DataLoader(\n",
        "        _DS(),\n",
        "        batch_size=batch_size, shuffle=shuffle,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "        persistent_workers=False, collate_fn=_collate\n",
        "    )\n",
        "\n",
        "train_dl = make_loader_from_wavs(df_tr,  BATCH_SIZE,   True,  NUM_WORKERS_TR, augment=True)\n",
        "val_dl   = make_loader_from_wavs(df_val, VAL_BATCH_SIZE, False, NUM_WORKERS_VA, augment=False)\n",
        "test_dl  = make_loader_from_wavs(df_te,  VAL_BATCH_SIZE, False, NUM_WORKERS_VA, augment=False)\n",
        "\n",
        "steps_tr = math.ceil(len(df_tr)/BATCH_SIZE) if train_dl else 0\n",
        "steps_va = math.ceil(len(df_val)/VAL_BATCH_SIZE) if val_dl else 0\n",
        "steps_te = math.ceil(len(df_te)/VAL_BATCH_SIZE) if test_dl else 0\n",
        "print(f\"[READY] fixed_seconds={FIXED_SECONDS}s  batch={BATCH_SIZE}/{VAL_BATCH_SIZE}  workers={NUM_WORKERS_TR}/{NUM_WORKERS_VA}\")\n",
        "\n",
        "# (6) 모델/학습 루프\n",
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "amp_dtype = None\n",
        "if device==\"cuda\":\n",
        "    if torch.cuda.is_bf16_supported(): amp_dtype=torch.bfloat16\n",
        "    else: amp_dtype=torch.float16\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try: torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception: pass\n",
        "\n",
        "if train_dl or val_dl:\n",
        "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "        \"facebook/wav2vec2-base\",\n",
        "        num_labels=3, id2label=id2label, label2id=label2id\n",
        "    ).to(device)\n",
        "\n",
        "    if FREEZE_BACKBONE:\n",
        "        for name, p in model.named_parameters():\n",
        "            if not (name.startswith(\"classifier.\") or name.startswith(\"projector.\")):\n",
        "                p.requires_grad = False\n",
        "        lr = LR_HEAD\n",
        "    else:\n",
        "        lr = LR_FULL\n",
        "\n",
        "    # optimizer 구성 (필수)\n",
        "    head_keys = [\"classifier\", \"projector\"]\n",
        "    params_head, params_body = [], []\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if any(k in name for k in head_keys):\n",
        "            params_head.append(p)\n",
        "        else:\n",
        "            params_body.append(p)\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {\"params\": params_head, \"lr\": LR_HEAD},\n",
        "        {\"params\": params_body, \"lr\": LR_FULL},\n",
        "    ], weight_decay=0.01)\n",
        "\n",
        "    # warmup 스케줄\n",
        "    def warmup_lambda(step):\n",
        "        if step < WARMUP_STEPS:\n",
        "            return float(step) / float(max(1, WARMUP_STEPS))\n",
        "        return 1.0\n",
        "\n",
        "    scheduler = LambdaLR(optimizer, warmup_lambda)\n",
        "\n",
        "    scaler = torch.amp.GradScaler(\"cuda\")\n",
        "\n",
        "    class EMA:\n",
        "        def __init__(self, alpha=0.2): self.alpha=alpha; self.sps=None; self.t=None\n",
        "        def tick(self, n):\n",
        "            now=time.time()\n",
        "            if self.t is None: self.t=now; return\n",
        "            dt=max(now-self.t,1e-6); inst=n/dt\n",
        "            self.sps = inst if self.sps is None else (self.alpha*inst+(1-self.alpha)*self.sps)\n",
        "            self.t=now\n",
        "        def sec_per_step(self, bs):\n",
        "            if not self.sps or self.sps<=0: return float(\"inf\")\n",
        "            return bs/self.sps\n",
        "    ema_tr, ema_va = EMA(0.2), EMA(0.2)\n",
        "    total_steps_all = EPOCHS*(steps_tr+steps_va)\n",
        "\n",
        "    def pretty_eta(sec: float) -> str:\n",
        "        if sec is None or not math.isfinite(sec): return \"estimating...\"\n",
        "        m, s = divmod(int(sec+0.5), 60); h, m = divmod(m, 60)\n",
        "        return f\"{h}h {m:02d}m {s:02d}s\" if h else (f\"{m}m {s:02d}s\" if m else f\"{s}s\")\n",
        "\n",
        "    # no_grad 블록 제거 + train/valid 모드 분리\n",
        "\n",
        "    def run_epoch(dl, train=True, epoch_idx=0):\n",
        "        if dl is None or dl.dataset is None:\n",
        "            print(f\"Skip {'train' if train else 'valid'} (empty)\")\n",
        "            return float('inf')\n",
        "\n",
        "        model.train(train)\n",
        "        phase = \"train\" if train else \"valid\"\n",
        "        steps_phase = steps_tr if train else steps_va\n",
        "        bs_phase    = BATCH_SIZE if train else VAL_BATCH_SIZE\n",
        "        tot_loss, seen, t0 = 0.0, 0, time.time()\n",
        "        seen_total = 0\n",
        "\n",
        "        ctx = nullcontext() if not amp_dtype else torch.autocast(\n",
        "            device_type=device, dtype=amp_dtype, enabled=(device==\"cuda\")\n",
        "        )\n",
        "\n",
        "        # ★ train이 아닐 때만 no_grad 사용\n",
        "        grad_ctx = nullcontext() if train else torch.inference_mode()\n",
        "\n",
        "        with grad_ctx:\n",
        "            for step, batch in enumerate(dl, 1):\n",
        "                if batch.get(\"empty\", False):\n",
        "                    continue\n",
        "\n",
        "                xb = batch[\"input_values\"].to(device, non_blocking=True)\n",
        "                am = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
        "                yb = batch[\"labels\"].to(device, non_blocking=True)\n",
        "\n",
        "                with ctx:\n",
        "                    out  = model(input_values=xb, attention_mask=am, labels=yb)\n",
        "                    loss = out.loss\n",
        "\n",
        "                if train:\n",
        "                    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                    if device == \"cuda\":\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "                        optimizer.step()\n",
        "\n",
        "                    scheduler.step()\n",
        "\n",
        "                bs = xb.size(0)\n",
        "                tot_loss += loss.item() * bs\n",
        "                seen += bs\n",
        "                seen_total += bs\n",
        "                (ema_tr if train else ema_va).tick(bs)\n",
        "\n",
        "                if device == \"cuda\" and (step % EMPTY_CACHE_EVERY == 0):\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                now = time.time()\n",
        "                if now - t0 >= LOG_EVERY_SEC:\n",
        "                    sps = seen / max(now - t0, 1e-6)\n",
        "                    sec_per = (ema_tr if train else ema_va).sec_per_step(bs_phase)\n",
        "                    done_all = (epoch_idx*(steps_tr+steps_va)) + (step if train else steps_tr + step)\n",
        "                    pct = 100.0 * done_all / max(1, total_steps_all)\n",
        "                    print(f\"[{phase} E{epoch_idx+1}/{EPOCHS} step {step}/{steps_phase}] \"\n",
        "                          f\"loss={tot_loss/max(1,seen):.4f}  speed={sps:.1f} samp/s  \"\n",
        "                          f\"prog={pct:.1f}%  ETA(phase)={pretty_eta((steps_phase-step)*sec_per)}\")\n",
        "                    t0 = now; seen = 0\n",
        "\n",
        "                del xb, am, yb, out, loss\n",
        "\n",
        "        return tot_loss / max(1, seen_total)\n",
        "\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        tr_loss = run_epoch(train_dl, True, ep)\n",
        "        if device==\"cuda\": torch.cuda.empty_cache(); gc.collect()\n",
        "        va_loss = run_epoch(val_dl, False, ep)\n",
        "        if device==\"cuda\": torch.cuda.empty_cache(); gc.collect()\n",
        "        print(f\"[E{ep}] train={tr_loss:.4f}  valid={va_loss:.4f}\")\n",
        "\n",
        "    # (7) 테스트 평가 + 시각화 (전체 test_dl)\n",
        "    def eval_collect(dl):\n",
        "        if dl is None or dl.dataset is None:\n",
        "            return None, None, None\n",
        "        model.eval()\n",
        "        losses=[]; y_true=[]; logits_all=[]\n",
        "        with torch.inference_mode(), (torch.autocast(device_type=device, dtype=amp_dtype, enabled=(device==\"cuda\")) if amp_dtype else nullcontext()):\n",
        "            for batch in dl:\n",
        "                if batch.get(\"empty\", False): continue\n",
        "                xb = batch[\"input_values\"].to(device, non_blocking=True)\n",
        "                am = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
        "                yb = batch[\"labels\"].to(device, non_blocking=True)\n",
        "                out = model(input_values=xb, attention_mask=am, labels=yb)\n",
        "                losses.append(out.loss.item())\n",
        "                logits_all.append(out.logits.float().detach().cpu().numpy())  # bf16→fp32\n",
        "                y_true.extend(yb.cpu().tolist())\n",
        "        logits = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0, len(class_names)))\n",
        "        return float(np.mean(losses)) if losses else float('inf'), np.array(y_true), logits\n",
        "\n",
        "    te_loss, y_true, logits = eval_collect(test_dl)\n",
        "    out_eval_dir = Path(f\"{SAVE_DIR}/eval_reports_train_3class_nocache\")\n",
        "    out_eval_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if y_true is None or y_true.size == 0:\n",
        "        print(\"[TEST] no test samples\")\n",
        "    else:\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "        y_pred = probs.argmax(axis=1)\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec_ma, rec_ma, f1_ma, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\",  zero_division=0)\n",
        "        prec_mi, rec_mi, f1_mi, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\",  zero_division=0)\n",
        "        prec_w,  rec_w,  f1_w,  _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "        labels_order = list(range(len(class_names)))\n",
        "        report = classification_report(\n",
        "            y_true, y_pred, labels=labels_order, target_names=class_names,\n",
        "            zero_division=0, output_dict=True\n",
        "        )\n",
        "        rep_df = pd.DataFrame(report).T\n",
        "        rep_csv = out_eval_dir / \"test_classification_report.csv\"\n",
        "        rep_df.to_csv(rep_csv)\n",
        "\n",
        "        # Robust binarize (항상 (N,3)로)\n",
        "        Y_bin = label_binarize(y_true, classes=labels_order)\n",
        "        if Y_bin.ndim == 1 or Y_bin.shape[1] != len(class_names):\n",
        "            # 부족하면 더미 열 채우기\n",
        "            pad = len(class_names) - (Y_bin.shape[1] if Y_bin.ndim > 1 else 1)\n",
        "            if Y_bin.ndim == 1:\n",
        "                Y_bin = np.column_stack([1 - Y_bin, Y_bin])\n",
        "                pad = len(class_names) - Y_bin.shape[1]\n",
        "            if pad > 0:\n",
        "                Y_bin = np.hstack([Y_bin, np.zeros((Y_bin.shape[0], pad), dtype=Y_bin.dtype)])\n",
        "\n",
        "        # AP per class / mAP\n",
        "        ap_per_class = []\n",
        "        for c in range(len(class_names)):\n",
        "            ap_c = average_precision_score(Y_bin[:, c], probs[:, c]) if Y_bin[:, c].sum() > 0 else float(\"nan\")\n",
        "            ap_per_class.append(ap_c)\n",
        "        mAP = np.nanmean(ap_per_class)\n",
        "\n",
        "        # ROC-AUC(OVR)\n",
        "        try:\n",
        "            roc_macro = roc_auc_score(Y_bin, probs, average=\"macro\", multi_class=\"ovr\")\n",
        "        except Exception:\n",
        "            roc_macro = float(\"nan\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
        "        cm_norm = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "        def plot_cm(cm, labels, fname):\n",
        "            fig, ax = plt.subplots(figsize=(5,4), dpi=140)\n",
        "            im = ax.imshow(cm, interpolation='nearest')\n",
        "            ax.figure.colorbar(im, ax=ax)\n",
        "            ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
        "                   xticklabels=labels, yticklabels=labels, ylabel='True', xlabel='Predicted',\n",
        "                   title='Confusion Matrix (normalized)')\n",
        "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "            thresh = np.nanmax(cm) * 0.6\n",
        "            for i in range(cm.shape[0]):\n",
        "                for j in range(cm.shape[1]):\n",
        "                    val = cm[i, j]\n",
        "                    ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
        "                            color=\"white\" if val > thresh else \"black\")\n",
        "            fig.tight_layout(); fig.savefig(fname, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "        cm_png = out_eval_dir / \"test_confusion_matrix_norm.png\"\n",
        "        plot_cm(cm_norm, class_names, cm_png)\n",
        "\n",
        "        def plot_pr(Y, P, labels, out_png):\n",
        "            fig, ax = plt.subplots(figsize=(6,5), dpi=140)\n",
        "            for c in range(P.shape[1]):\n",
        "                if Y[:, c].sum() == 0: continue\n",
        "                prec, rec, _ = precision_recall_curve(Y[:, c], P[:, c])\n",
        "                ap_c = average_precision_score(Y[:, c], P[:, c])\n",
        "                ax.plot(rec, prec, label=f\"{labels[c]} (AP={ap_c:.3f})\")\n",
        "            ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\"); ax.set_title(\"Precision–Recall Curves (Test)\")\n",
        "            ax.legend(); fig.tight_layout(); fig.savefig(out_png, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "        def plot_roc(Y, P, labels, out_png):\n",
        "            fig, ax = plt.subplots(figsize=(6,5), dpi=140)\n",
        "            for c in range(P.shape[1]):\n",
        "                if Y[:, c].sum() == 0: continue\n",
        "                fpr, tpr, _ = roc_curve(Y[:, c], P[:, c])\n",
        "                auc_c = np.trapz(tpr, fpr)\n",
        "                ax.plot(fpr, tpr, label=f\"{labels[c]} (AUC~={auc_c:.3f})\")\n",
        "            ax.plot([0,1],[0,1],'--')\n",
        "            ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.set_title(\"ROC Curves (OVR, Test)\")\n",
        "            ax.legend(); fig.tight_layout(); fig.savefig(out_png, bbox_inches=\"tight\"); plt.close(fig)\n",
        "\n",
        "        pr_png  = out_eval_dir / \"test_pr_curves.png\"\n",
        "        roc_png = out_eval_dir / \"test_roc_curves.png\"\n",
        "        plot_pr(Y_bin, probs, class_names, pr_png)\n",
        "        plot_roc(Y_bin, probs, class_names, roc_png)\n",
        "\n",
        "        np.savez_compressed(out_eval_dir / \"test_raw_outputs.npz\", logits=logits, y_true=y_true)\n",
        "\n",
        "        summary = {\n",
        "            \"samples\": int(len(y_true)),\n",
        "            \"classes\": class_names,\n",
        "            \"test_loss\": float(te_loss),\n",
        "            \"accuracy\": float(acc),\n",
        "            \"precision_macro\": float(prec_ma),\n",
        "            \"recall_macro\": float(rec_ma),\n",
        "            \"f1_macro\": float(f1_ma),\n",
        "            \"precision_micro\": float(prec_mi),\n",
        "            \"recall_micro\": float(rec_mi),\n",
        "            \"f1_micro\": float(f1_mi),\n",
        "            \"precision_weighted\": float(prec_w),\n",
        "            \"recall_weighted\": float(rec_w),\n",
        "            \"f1_weighted\": float(f1_w),\n",
        "            \"AP_per_class\": {\n",
        "                class_names[i]: (None if np.isnan(ap_per_class[i]) else float(ap_per_class[i]))\n",
        "                for i in range(len(class_names))\n",
        "            },\n",
        "            \"mAP\": (None if np.isnan(mAP) else float(mAP)),\n",
        "            \"roc_auc_macro_ovr\": (None if (roc_macro!=roc_macro) else float(roc_macro)),\n",
        "            \"report_csv\": str(rep_csv),\n",
        "            \"cm_png\": str(cm_png),\n",
        "            \"pr_png\": str(pr_png),\n",
        "            \"roc_png\": str(roc_png),\n",
        "            \"raw_npz\": str(out_eval_dir / \"test_raw_outputs.npz\"),\n",
        "        }\n",
        "        with open(out_eval_dir / \"test_metrics_summary.json\", \"w\") as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(\"\\n=== TEST SUMMARY (3-class) ===\")\n",
        "        for k, v in summary.items():\n",
        "            if isinstance(v, float): print(f\"{k:>24}: {v:.4f}\")\n",
        "            else:                    print(f\"{k:>24}: {v}\")\n",
        "        print(f\"\\n✔ 결과 저장: {out_eval_dir}\")\n",
        "\n",
        "    # (8) 모델 저장\n",
        "    model_dir  = f\"{SAVE_DIR}/wav2vec2_clf_model3\"\n",
        "    state_path = f\"{SAVE_DIR}/wav2vec2_clf3.pt\"\n",
        "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(model_dir)\n",
        "    torch.save(model.state_dict(), state_path)\n",
        "    print(f\"\\n✔ 모델 저장 완료: {model_dir} & {state_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping training: no train/val data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "R5MoqCApM0JZ",
        "outputId": "9103df48-5c6b-4be1-f5cb-5fcb5376c166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[orig] metadata.csv 존재: /content/drive/MyDrive/origin/metadata.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[before balance] {'tts': 254300, 'tts_gsm': 254292, 'orig': 54066}\n",
            "[after balance] {'orig': 54066, 'tts': 54066, 'tts_gsm': 54066}\n",
            "[labels] kept types: {'orig': 54066, 'tts': 54066, 'tts_gsm': 54066}\n",
            "[total] 162,198 rows  FAST_SCAN=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[split] train=77,853  val=19,461  test=0\n",
            "[by_type] train: {'orig': 25951, 'tts': 25951, 'tts_gsm': 25951}\n",
            "[by_type] val  : {'orig': 6487, 'tts': 6487, 'tts_gsm': 6487}\n",
            "[by_type] test : {}\n",
            "[READY] fixed_seconds=2s  batch=16/16  workers=0/0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
            "  return data.pin_memory(device)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
            "  return data.pin_memory(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train E1/5 step 1/4866] loss=1.0938  speed=0.9 samp/s  prog=0.0%  ETA(phase)=estimating...\n",
            "[train E1/5 step 2/4866] loss=2.2824  speed=0.8 samp/s  prog=0.0%  ETA(phase)=27h 54m 43s\n",
            "[train E1/5 step 3/4866] loss=3.5529  speed=0.5 samp/s  prog=0.0%  ETA(phase)=30h 26m 15s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3481367249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mva_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3481367249.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(dl, train, epoch_idx)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgrad_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3481367249.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torchaudio\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, accuracy_score,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 경로 설정\n",
        "# ======================\n",
        "BASE_TEST_DIR = \"/content/drive/MyDrive/test\"\n",
        "MODEL_DIR     = \"/content/drive/MyDrive/tts_outputs/wav2vec2_clf_model2\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"[INFO] Loading model from\", MODEL_DIR)\n",
        "\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
        "model.eval()\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "label_map = {\"orig\": 0, \"tts\": 1, \"tts_gsm\": 2}\n",
        "id2label  = {v: k for k, v in label_map.items()}\n",
        "\n",
        "\n",
        "# ======================\n",
        "# WAV 로더\n",
        "# ======================\n",
        "def load_wav_safe(path, target_sr=16000):\n",
        "    wav, sr = sf.read(path, dtype=\"float32\", always_2d=False)\n",
        "    wav = torch.tensor(wav, dtype=torch.float32)\n",
        "\n",
        "    if wav.ndim > 1:\n",
        "        wav = wav.mean(dim=-1)\n",
        "\n",
        "    if sr != target_sr:\n",
        "        wav = torchaudio.functional.resample(wav.unsqueeze(0), sr, target_sr).squeeze(0)\n",
        "\n",
        "    return wav\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 예측 함수\n",
        "# ======================\n",
        "def predict_logits(wav):\n",
        "    inputs = processor(wav, sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\")\n",
        "    input_values = inputs[\"input_values\"]\n",
        "\n",
        "    # attention mask 직접 생성\n",
        "    attention_mask = torch.ones_like(input_values, dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            input_values=input_values.to(device),\n",
        "            attention_mask=attention_mask.to(device)\n",
        "        ).logits\n",
        "\n",
        "    return logits.cpu().numpy()[0]  # 1D logits\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 폴더 테스트 (10% 샘플링)\n",
        "# ======================\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_prob = []   # softmax probability 저장\n",
        "\n",
        "SAMPLE_FRAC = 0.5\n",
        "\n",
        "for class_name in [\"orig\", \"tts\", \"tts_gsm\"]:\n",
        "    folder = Path(BASE_TEST_DIR) / class_name\n",
        "    if not folder.exists():\n",
        "        print(f\"[WARN] 폴더 없음: {folder}\")\n",
        "        continue\n",
        "\n",
        "    file_list = list(folder.glob(\"*.wav\")) + list(folder.glob(\"*.mp3\"))\n",
        "    total_files = len(file_list)\n",
        "\n",
        "    k = max(1, int(total_files * SAMPLE_FRAC))\n",
        "    file_list = random.sample(file_list, k)\n",
        "    print(f\"[TEST] {class_name}: total={total_files}, sampled={k}\")\n",
        "\n",
        "    for f in file_list:\n",
        "        try:\n",
        "            wav = load_wav_safe(str(f))\n",
        "            logits = predict_logits(wav)\n",
        "            probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
        "\n",
        "            pred = np.argmax(probs)\n",
        "\n",
        "            y_true.append(label_map[class_name])\n",
        "            y_pred.append(pred)\n",
        "            y_prob.append(probs)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERR] {f}: {e}\")\n",
        "\n",
        "\n",
        "# numpy 변환\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_prob = np.array(y_prob)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 기본 지표\n",
        "# ======================\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"\\n전체 Accuracy:\", accuracy)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# normalized confusion matrix\n",
        "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Confusion Matrix 시각화 (normalized, softmax 기반)\n",
        "# ======================\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(cm_norm, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Normalized)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks([0,1,2], [\"orig\",\"tts\",\"tts_gsm\"])\n",
        "plt.yticks([0,1,2], [\"orig\",\"tts\",\"tts_gsm\"])\n",
        "\n",
        "for i in range(cm_norm.shape[0]):\n",
        "    for j in range(cm_norm.shape[1]):\n",
        "        plt.text(j, i, f\"{cm_norm[i, j]:.2f}\", ha=\"center\", va=\"center\",\n",
        "                 color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ROC / AUC 계산\n",
        "# ======================\n",
        "y_bin = label_binarize(y_true, classes=[0,1,2])  # One-hot encoding\n",
        "\n",
        "auc_macro = roc_auc_score(y_bin, y_prob, average=\"macro\", multi_class=\"ovr\")\n",
        "print(\"\\nMacro AUC:\", auc_macro)\n",
        "\n",
        "# ======================\n",
        "# ROC Curve 그리기\n",
        "# ======================\n",
        "plt.figure(figsize=(6, 5))\n",
        "\n",
        "for i, name in enumerate([\"orig\", \"tts\", \"tts_gsm\"]):\n",
        "    fpr, tpr, _ = roc_curve(y_bin[:, i], y_prob[:, i])\n",
        "    auc_c = roc_auc_score(y_bin[:, i], y_prob[:, i])\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_c:.3f})\")\n",
        "\n",
        "plt.plot([0,1],[0,1],'--', color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves (One-vs-Rest)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Overall Accuracy 이미지 출력\n",
        "# ======================\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.text(0.5, 0.5, f\"Overall Accuracy: {accuracy:.4f}\\nMacro AUC: {auc_macro:.4f}\",\n",
        "         ha=\"center\", va=\"center\", fontsize=17)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127,
          "referenced_widgets": [
            "bfe3029a111742248b819ea6ca3d2247",
            "3c6874d3bbc840e495ccf42b896845d1",
            "40523396f1644770a343e61a915add88",
            "2f25b534f1944f1dbd451258c7a6182e"
          ]
        },
        "id": "joG00JhLymFm",
        "outputId": "1f2ea157-7603-4e8d-f202-0d79f4fde97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loading model from /content/drive/MyDrive/tts_outputs/wav2vec2_clf_model2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfe3029a111742248b819ea6ca3d2247",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c6874d3bbc840e495ccf42b896845d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40523396f1644770a343e61a915add88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f25b534f1944f1dbd451258c7a6182e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] orig: total=5406, sampled=2703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "#   Wav2Vec2-base Baseline vs Fine-Tuned 3-class Model Performance\n",
        "#   Test Dataset = 전체 데이터 중 10% 자동 분리\n",
        "#   단일 셀 전체 코드\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# ---------------------------\n",
        "# 설정\n",
        "# ---------------------------\n",
        "\n",
        "# 데이터 전체 루트 (orig/tts/tts_gsm 폴더가 있어야 함)\n",
        "DATA_ROOT = \"/content/drive/MyDrive/test\"       # 예: 전체 데이터 위치\n",
        "FINETUNED_MODEL_DIR = \"/content/drive/MyDrive/tts_outputs/wav2vec2_clf_model2\"    # 너의 fine-tuned 모델 dir\n",
        "\n",
        "labels = [\"orig\", \"tts\", \"tts_gsm\"]\n",
        "label_map = {\"orig\": 0, \"tts\": 1, \"tts_gsm\": 2}\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------------------------\n",
        "# 전체 파일 목록 불러오기\n",
        "# ---------------------------\n",
        "paths = []\n",
        "y = []\n",
        "\n",
        "for cls in labels:\n",
        "    folder = os.path.join(DATA_ROOT, cls)\n",
        "    for fname in os.listdir(folder):\n",
        "        if fname.lower().endswith(\".wav\"):\n",
        "            paths.append(os.path.join(folder, fname))\n",
        "            y.append(label_map[cls])\n",
        "\n",
        "paths = np.array(paths)\n",
        "y = np.array(y)\n",
        "\n",
        "# ---------------------------\n",
        "# 데이터 10%를 test로 분리\n",
        "# ---------------------------\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    paths,\n",
        "    y,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"전체 데이터: {len(paths)}개\")\n",
        "print(f\"테스트 데이터: {len(test_paths)}개 (10%)\")\n",
        "print(f\"학습 데이터: {len(train_paths)}개 (사용 안 함, 단지 분리만 수행)\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Baseline 모델 정의: Wav2Vec2-base + 랜덤 Linear Head\n",
        "# ================================================================\n",
        "\n",
        "class W2V2_Baseline(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "        hidden = self.backbone.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden, 3)  # 3-class random head\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None):\n",
        "        out = self.backbone(input_values=input_values, attention_mask=attention_mask)\n",
        "        hidden_states = out.last_hidden_state      # [B, T, H]\n",
        "        pooled = hidden_states.mean(dim=1)         # [B, H]\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "\n",
        "baseline_model = W2V2_Baseline().to(device)\n",
        "baseline_model.eval()\n",
        "\n",
        "finetuned_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "    FINETUNED_MODEL_DIR\n",
        ").to(device)\n",
        "finetuned_model.eval()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 평가 함수\n",
        "# ================================================================\n",
        "def evaluate(model, model_name):\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_prob = []\n",
        "\n",
        "    print(f\"\\n\\n===== {model_name} 평가 시작 =====\")\n",
        "\n",
        "    for path, label in zip(test_paths, test_labels):\n",
        "\n",
        "        wav, sr = librosa.load(path, sr=16000)\n",
        "\n",
        "        inputs = processor(\n",
        "            wav,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # ----- baseline 모델(직접 정의) -----\n",
        "            if hasattr(outputs, \"logits\"):       # fine-tuned 모델\n",
        "                logits = outputs.logits\n",
        "            else:                                # baseline 모델\n",
        "                logits = outputs\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "            pred = np.argmax(probs)\n",
        "\n",
        "        y_true.append(label)\n",
        "        y_pred.append(pred)\n",
        "        y_prob.append(probs)\n",
        "\n",
        "    y_prob = np.array(y_prob)\n",
        "    y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
        "\n",
        "    auc_macro = roc_auc_score(y_true_bin, y_prob, average=\"macro\")\n",
        "    auc_micro = roc_auc_score(y_true_bin, y_prob, average=\"micro\")\n",
        "\n",
        "    print(\"AUC-macro:\", auc_macro)\n",
        "    print(\"AUC-micro:\", auc_micro)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=labels))\n",
        "\n",
        "    return auc_macro, auc_micro\n",
        "\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 실행\n",
        "# ================================================================\n",
        "baseline_auc_macro, baseline_auc_micro = evaluate(baseline_model, \"Baseline (Wav2Vec2-base + Random Head)\")\n",
        "fine_auc_macro, fine_auc_micro = evaluate(finetuned_model, \"Fine-Tuned Model\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 최종 비교 요약 출력\n",
        "# ================================================================\n",
        "print(\"\\n\\n===== 최종 성능 비교 요약 =====\")\n",
        "print(f\"Baseline AUC-macro: {baseline_auc_macro}\")\n",
        "print(f\"Fine-Tuned AUC-macro: {fine_auc_macro}\")\n",
        "\n",
        "improvement = (fine_auc_macro - baseline_auc_macro) / baseline_auc_macro * 100\n",
        "print(f\"\\nAUC-macro 성능 향상률: {improvement:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlB1RWSYFFqg",
        "outputId": "dbbef7d0-8a52-497c-f633-4d3d09ae3cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 데이터: 39587개\n",
            "테스트 데이터: 3959개 (10%)\n",
            "학습 데이터: 35628개 (사용 안 함, 단지 분리만 수행)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "===== Baseline (Wav2Vec2-base + Random Head) 평가 시작 =====\n",
            "AUC-macro: 0.6491372652856797\n",
            "AUC-micro: 0.5242667947625067\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        orig       0.07      0.01      0.02       541\n",
            "         tts       0.00      0.00      0.00      1857\n",
            "     tts_gsm       0.39      0.98      0.56      1561\n",
            "\n",
            "    accuracy                           0.39      3959\n",
            "   macro avg       0.16      0.33      0.19      3959\n",
            "weighted avg       0.16      0.39      0.22      3959\n",
            "\n",
            "\n",
            "\n",
            "===== Fine-Tuned Model 평가 시작 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-macro: 0.9999990322308209\n",
            "AUC-micro: 0.9999993938883917\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        orig       1.00      1.00      1.00       541\n",
            "         tts       1.00      1.00      1.00      1857\n",
            "     tts_gsm       1.00      1.00      1.00      1561\n",
            "\n",
            "    accuracy                           1.00      3959\n",
            "   macro avg       1.00      1.00      1.00      3959\n",
            "weighted avg       1.00      1.00      1.00      3959\n",
            "\n",
            "\n",
            "\n",
            "===== 최종 성능 비교 요약 =====\n",
            "Baseline AUC-macro: 0.6491372652856797\n",
            "Fine-Tuned AUC-macro: 0.9999990322308209\n",
            "\n",
            "AUC-macro 성능 향상률: 54.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    }
  ]
}